import arxiv  # ArXiv akademik makale arama kÃ¼tÃ¼phanesi
import feedparser  # RSS kaynaklarÄ±ndan haber Ã§ekmek iÃ§in kullanÄ±lÄ±r
import requests  # HTTP istekleri yapmak iÃ§in kullanÄ±lÄ±r
from bs4 import BeautifulSoup  # Web kazÄ±ma (scraping) iÃ§in HTML ayrÄ±ÅŸtÄ±rÄ±cÄ±
from datetime import datetime  # Tarih bilgisi almak iÃ§in

# ----------- 1. Fetch Academic Papers from arXiv -----------
# ArXiv'den verilen konuya uygun akademik makaleleri getirir
def fetch_arxiv_papers(query="hydrogen energy", max_results=3):
    search = arxiv.Search(
        query=query,  # Arama yapÄ±lacak konu baÅŸlÄ±ÄŸÄ±
        max_results=max_results,  # KaÃ§ sonuÃ§ getirileceÄŸi
        sort_by=arxiv.SortCriterion.SubmittedDate  # En yeni tarihe gÃ¶re sÄ±rala
    )
    papers = []
    for result in search.results():  # SonuÃ§larÄ± dÃ¶n
        papers.append({
            "title": result.title.strip(),  # Makale baÅŸlÄ±ÄŸÄ±
            "summary": result.summary.strip(),  # Makale Ã¶zeti
            "url": result.entry_id  # Makale baÄŸlantÄ±sÄ±
        })
    return papers

# ----------- 2. Fetch News from Google News RSS -----------
# Google News Ã¼zerinden ilgili konudaki haberleri Ã§eker
def fetch_google_news(query="hydrogen energy", max_articles=3):
    url = f"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(url)  # RSS verisini parse et
    news_items = []
    for entry in feed.entries[:max_articles]:  # Belirtilen sayÄ±da haber al
        news_items.append({
            "title": entry.title.strip(),  # Haber baÅŸlÄ±ÄŸÄ±
            "summary": entry.summary.strip(),  # Haber Ã¶zeti
            "link": entry.link  # Haber baÄŸlantÄ±sÄ±
        })
    return news_items

# ----------- 3. Scrape Company News from Plug Power -----------
# Plug Power ÅŸirketinin web sitesinden haberleri kazÄ±r
def scrape_plug_power_news(max_articles=3):
    url = "https://www.plugpower.com/news/"  # Haberlerin bulunduÄŸu sayfa
    response = requests.get(url)  # HTTP isteÄŸi gÃ¶nder
    soup = BeautifulSoup(response.text, "html.parser")  # HTML verisini ayrÄ±ÅŸtÄ±r
    articles = soup.select("div.news-list-item")  # Haber kartlarÄ±nÄ± seÃ§
    news_items = []
    for article in articles[:max_articles]:  # Ä°lk belirlenen sayÄ±yÄ± al
        title = article.select_one("h3").get_text(strip=True)
        summary = article.select_one("p").get_text(strip=True)
        link = article.find("a")["href"]
        news_items.append({
            "title": title,
            "summary": summary,
            "link": link
        })
    return news_items

# ----------- 4. Summarize using OpenRouter model -----------
# Toplanan verileri OpenRouter API kullanarak Ã¶zetletir
def summarize_items_openrouter(items, section_title, api_key):
    # Gelen tÃ¼m baÅŸlÄ±k + Ã¶zet metnini tek promptâ€™a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz
    combined_text = "\n\n".join([f"Title: {item['title']}\nSummary: {item['summary']}" for item in items])

    # OpenRouter modeline gÃ¶nderilecek mesaj
    prompt = f"""You're an energy analyst. Summarize the following {section_title.lower()} into 3â€“5 concise bullet points:\n\n{combined_text}"""

    # API'ye Ã¶zetleme isteÄŸi gÃ¶nderiyoruz
    response = requests.post(
        "https://openrouter.ai/api/v1/chat/completions",
        json={
            "model": "qwen/qwen2.5-vl-32b-instruct:free",  # KullanÄ±lan model
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        },
        headers={
            "Authorization": f"Bearer {api_key}"  # API anahtarÄ±nÄ± gÃ¶nderiyoruz
        }
    )

    # Gelen cevaptan Ã¶zet metni Ã§Ä±kar
    result = response.json()
    return result["choices"][0]["message"]["content"].strip()

# ----------- 5. Build the Daily Report -----------
# TÃ¼m bÃ¶lÃ¼mleri toplayÄ±p gÃ¼nlÃ¼k bir rapor oluÅŸturur
def build_daily_report(api_key):
    today = datetime.today().strftime('%Y-%m-%d')  # BugÃ¼nÃ¼n tarihi
    report = f"# ğŸ”‹ Daily Hydrogen Energy Report ({today})\n\n"  # Rapor baÅŸlÄ±ÄŸÄ±

    print("Fetching academic papers from arXiv...")
    arxiv_data = fetch_arxiv_papers()

    print("Fetching news from Google News...")
    google_news_data = fetch_google_news()

    print("Scraping Plug Power company news...")
    plug_news_data = scrape_plug_power_news()

    print("Summarizing arXiv papers with OpenRouter...")
    arxiv_summary = summarize_items_openrouter(arxiv_data, "Academic Papers", api_key)
    report += "## ğŸ“˜ Academic Papers (arXiv)\n" + arxiv_summary + "\n\n"

    print("Summarizing news articles with OpenRouter...")
    news_summary = summarize_items_openrouter(google_news_data, "News Articles", api_key)
    report += "## ğŸ— News Articles (Google News)\n" + news_summary + "\n\n"

    print("Summarizing Plug Power news with OpenRouter...")
    company_summary = summarize_items_openrouter(plug_news_data, "Company News", api_key)
    report += "## ğŸ­ Company Update: Plug Power\n" + company_summary + "\n\n"

    return report

# ----------- 6. Run It! -----------
if __name__ == "__main__":
    # âŒ GÃ¼venlik uyarÄ±sÄ±: API anahtarÄ±nÄ± kod iÃ§inde bÄ±rakma!
    API_KEY = "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"
    # Ana raporu Ã¼ret
    final_report = build_daily_report(API_KEY)
    print(final_report)
